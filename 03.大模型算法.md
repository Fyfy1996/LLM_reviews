## 1.在指令微调中，如何设置、选择和优化不同的超参数，以及其对模型效果的影响？

无明确定论，总结一下分类别下的参数

* 数据集
  * drop-last
* 模型本身
* 训练设置
  * batchsize
  * clip grad
  * lr rate
  * weight decay
  * loss scale
  * warm up iters
  * lr decay style
  * lr decay iters

## 2.在指令微调中，如何选择最佳的指令策略，以及其对模型效果的影响？

总的来说，你需要先明确自己需要向什么方面微调：是一个更对齐人类价值观的gossip bot？还是更具备解决问题能力的helper？是更健谈并回答内容更丰富？还是人狠话不多？是单领域？还是多领域？

a.数据需要什么样的？

b.该如何组织数据集来微调？

首先是一些已知的要点：

Flanv2提到的一些tricks【待补充】

* 更多样的任务-使用不同的提示模板
* 混合few-shot和zero-shot learning
* 考虑输入反转
* 加入CoT
* 任务混合权重按照经验考虑即可

下面是数据集组织的方法论：

c.该如何评估sft后的model的能力？

## 3.llama, glm，bloom等现有大模型的数据处理，训练细节，以及不足之处模型架构的优化点，包括但不限于attention, norm, embedding

### LLAMA

见"03.03.各个大模型论文细节.md"

## 4.解决显存不够的方法有哪些？

### 如果只做推理

大致估算，每1B参数需要4G的显存（float32下），float16/int8/int4对应除以2就行

除参数的开销外，前向传递过程中有一小部分额外开销。一般这种开销小于等于20%。

针对参数的部分，有以下的方法：

a. 模型压缩，包括量化、蒸馏、剪枝、Moefication

b. Memory Offload，部分显存挪去内存，只是内存确实跑不动。

c.并行

### 微调

20$Phi$

**5.请解释P-tuning 的工作原理，并说明它与传统的 fine-tuning方法的不同之处。**

**6.介绍一下Prefix-tuning的思想和应用场景，以及它如何解决一些NLP任务中的挑战**

**7.Lora的原理和存在的问题讲一下？**

原理：

利用LLM模型参数的低秩属性（过参数化）通过先降维再升维的方法，模拟参数的改变，实现极小参数量finetune

$$
h = W_0 x + \Delta W x = W_0 x + BA x

$$

做法：

LoRA只应用于Attention模块中的4种权重矩

保证权重矩阵的种类的数量比起增加隐藏层维度r（一般4，8，16）更为重要，增加r并不一定能覆盖更加有意义的子空间。

缺点：

a.基于低秩的微调可能并不always work，比如finetune与pretrain的gap过大的时候，比如中英差异。当然，这一点在LLM时代可能并不突出，我们认为LLM在预训练阶段已经get了所有基本的知识，finetune只是格式微调，因此可能不会有上述gap过大的情况。

b.用lora也需要设置r和target module等，这部分超参的设置需要考虑

**AdaLora**

调整增量矩分配

以奇异值分解的形式对增量更新进行参数化，并根据重要性指标裁剪掉不重要的奇异值，同时保留奇异向量*

$$
W = W^{(0)} + \Delta = W^{(0)} + P\Lambda Q

$$

QLora

4bit量化

工程上优化



**8.bf16，fp16半精度训练的优缺点**

**9.如何增加context length 模型训练中节约显存的技巧。**

**10.RLHF完整训练过程是什么？RL过程中涉及到几个模型？显存占用关系和SFT有什么区别？**

**11.RLHF过程中RM随着训练过程得分越来越高，效果就一定好吗？有没有极端情况？**

**12.encoder only，decoder only，encoder-decoder 划分的具体标注是什么？典型代表模型有哪些？**
