# LLAMA2

## 预训练

标准Transformer架构

RMSNorm归一化

SwiGLU激活函数

旋转位置嵌入RoPE

分组查询注意力GQA

字对编码（BPE）算法

## 评估

Llama 2 70B在MMLU和GSM8K上与GPT-3.5接近，但在编码基准测试中存在显著差距。Llama 2 70B在几乎所有基准测试中与PaLM（540B）相当或更好，但与GPT-4和PaLM-2-L之间仍存在较大差距。

## Finetuning

### SFT 有监督微调

结论：**Quality Is All You Need**，注释数量在数万个左右就足以获得高质量的结果，并在收集了总共27,540个注释后停止了SFT的注释。

### RLHF

他们选择了二元比较协议，因为它可以最大化收集提示的多样性。

他们在收集偏好注释时，关注的是有用性和安全性。有用性是指Llama 2-Chat响应如何满足用户的请求和提供所请求的信息；安全性是指Llama 2-Chat的响应是否不安全。

### Reward Modeling

训练了两个独立的奖励模型，一个针对帮助性（称为Helpfulness RM），另一个针对安全性（Safety RM）

**Scaling Trends** :结果显示较大规模的模型在类似数据量下获得更高的性能。此外，作者指出目前使用的数据注释量尚未达到平稳状态，这表明通过更多的注释可以进一步提高性能。值得注意的是，奖励模型的准确性是Llama 2-Chat最重要的衡量指标之一，因此，奖励模型的改进可以直接转化为Llama 2-Chat的改进。

### 多轮对话控制

在对话任务中，某些instruction可能需要在多轮对话中使用，但是作者在最初的RLHF中发现，模型在多轮对话后会遗忘对话开始的instruction。为了解决这个问题，作者提出了Ghost Attention (GAtt)方法——一种模型训练的trick，这种方法能够帮助模型在多轮对话中对知识进行聚焦。

Ghost Attention（GAtt）方法就是在原先的多轮对话中增加一个instruction，

将原始的对话变[u1,a1,…,un,an] （u1表示用户的第一轮输入，a1表示对应的第一轮回答，u2表示第二轮的用户输入，依次类推）变为[inst+u1,a1 ’ ,…,inst+un,an’ ]。

这种方式的思路就是生成一种更加聚焦的数据集来微调模型，比如原始的数据集为[u1,a1,…,un,an]，作者通过新增instructions 得到新的数据集[inst+u1,a1 ’ ,…,inst+un,an’ ]，

然后用这两者混合的数据[inst+u1,a1 ‘,u2,a2’ ,…,un,an’ ]来微调模型，这样就能使模型在多轮对话中始终保持对instruction的聚焦。

为了避免出现语句不匹配的问题，在训练过程中，仅仅保留第一轮的提示，并将中间轮数的损失置为0。

需要注意的是，作者并不是在RLHF阶段一开始就使用GAtt方法，而是在RLHF V3之后才使用GAtt方法使模型增加对多轮对话的控制。
