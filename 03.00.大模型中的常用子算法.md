## 近端策略优化Proximal Policy Optimization

### 强化学习分类

根据一种比较通行的分类法，强化学习可以分为基于值的方法、基于策略的方法和actor-critic方法这三类。

**基于值函数的学习方法**: 

Value-Based的方法输出的是动作的价值，选择价值最高的动作。适用于非连续的动作。常见的方法有Q-learning、Deep Q Network和Sarsa

**基于策略梯度的学习方法**

Policy-Based的方法直接输出下一步动作的概率，根据概率来选取动作。但不一定概率最高就会选择该动作，还是会从整体进行考虑。适用于非连续和连续的动作。

常见的方法有Policy gradients。一般在全连接层之后还有一个softmax层，从而将打分转化成概率。

**Actor-Critic**

融合了上述两种方法，价值函数和策略函数一起进行优化。价值函数负责在环境学习并提升自己的价值判断能力，而策略函数则接受价值函数的评价，尽量采取在价值函数那可以得到高分的策略。

### PPO算法的背景

PPO是用于策略梯度(Policy Gradient， PG)下的。PG的目的是是直接建模与优化Policy：

输入当前的状态，输出action的概率分布，依据概率分布选择下一个action。

### Policy gradient的具体过程

1.先有一个策略 $\theta$ ，通过这个策略收获很多数据，收集 s（state） 和 a（action）的成对资料，得到一堆游戏记录。即 $\tau^i : ( s^i, a^i)$ ，奖励是 $R^i $，

2.将资料带入公式，计算梯度: 也就是log probablility，$p_\theta(a_t|s_t)$,取梯度，乘上weight，就是当前策略的reward。

3.基于reward去更新模型，用更新后的模型获取数据，再更新模型

这里的PG算法，可以看到Agent不断和环境互动，然后学习更新，这个过程中policy是不变的。

因此，每次更新模型后，就需要重新采样数据，造成了PG花很多时间在采样数据上。

### PPO使用了重要性采样的思想

希望可以用一个旧策略收集到的数据来训练新策略，这意味着我们可以重复利用这些数据来更新策略多次，效率上可以提升很多。

具体来说，PPO算法利用重要性采样的思想，在不知道策略路径的概率p的情况下，通过模拟一个近似的q分布，只要p同q分布不差的太远，通过多轮迭代可以快速参数收敛

重要性采样公式：

$$
E_{x-p(x)}[f(x)] = \int p(xPf(x) dx\\
= \int \frac{q(x)}{q(x)} p(x)f(x) dx \\
= \int q(x) \frac{p(x)}{q(x)} f(x) dx \\
= E_{x-q(x)} [\frac{p(x)}{q(x)}f(x)]

$$

PPO重要的突破就是在于对新旧新旧策略器参数进行了约束，希望新的策略网络和旧策略网络的越接近越好。

近线策略优化的意思就是：新策略网络要利用到旧策略网络采样的数据进行学习，不希望这两个策略相差特别大，否则就会学偏。

但重要性采样依然存在一个问题。**我们用q代替了p来采样x**，但是两个随机变量的分布，即使**均值mean一样**，也**不代表方差variance一样。**

PPO中，用 $\theta'$ 代替 $\theta$ 进行采样，且不希望两者差距过大，因此在里面加入一个**约束项： $\beta KL(\theta ,\theta')$**

### PPO算法

算法的步骤如下：

1. 初始化Policy的参数 $\theta ^0$.
2. 在每次迭代中，首先使用 $\theta^k$ 与环境互动，得到一堆 $ (s_t, a_t)$ 数据对，计算优势函数，我们这里可以理解成奖励。
3. 而不同于PG算法只能应用一次数据，PPO可以多次训练 $\theta$。

这里还有一个trick，即动态更新 $\beta$。我们设置一个可以接受的最大、最小KL，成为适应性KL惩罚。如果目前的KL大于设定的KL最大值，说明惩罚项没有发挥效果，就加大 $\beta$；如果比最小值小，说明惩罚项太强了，就减少 $\beta$ 。

使用KL散度的是初版的PPO算法，因为要计算KL散度还是有点复杂，因此延伸出了clip-PPO算法。
