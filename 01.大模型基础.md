# 大模型基本概念

**1.**  **大模型** ： **一般指1亿以上参数的模型** ，但是这个标准一直在升级，目前万亿参数以上的模型也有了。大语言模型（Large Language Model，LLM）是针对语言的大模型。

**2.**  **175B、60B、540B等** ：这些一般指参数的个数， **B是Billion/十亿的意思** ， **175B是1750亿参数** ，这是ChatGPT大约的参数规模。

**3.**  **强化学习** ：（Reinforcement Learning）一种机器学习的方法，通过从**外部获得激励**来校正学习方向从而获得一种自适应的学习能力。

**4.**  **基于人工反馈的强化学习（RLHF）** ： **（Reinforcement Learning from Human Feedback）构建人类反馈数据集，训练一个激励模型** ，模仿人类偏好对结果打分，这是GPT-3后时代大语言模型越来越像人类对话核心技术。

**5.**  **涌现** ：（Emergence）或称创发、突现、呈展、演生，是一种现象。许多小实体相互作用后产生了大实体，而这个大实体展现了组成它的小实体所不具有的特性。研究发现，**模型规模达到一定阈值以上后，会在多步算术、大学考试、单词释义等场景的准确性显著提升，称为涌现。**

**6.**  **泛化** ：（Generalization）模型泛化是指一些模型可以 **应用（泛化）到其他场景** ，通常为采用迁移学习、微调等手段实现泛化。

**7.**  **微调** ：（FineTuning）针对大量数据训练出来的预训练模型，后期采用业务相关数据进一步训练原先模型的相关部分，得到准确度更高的模型，或者更好的泛化。

**8.**  **指令微调** ：（Instruction FineTuning），针对已经存在的预训练模型，给出额外的指令或者标注数据集来提升模型的性能。

**9.**  **思维链** ： **（Chain-of-Thought，CoT）** 。通过让大语言模型（LLM）将一个问题拆解为多个步骤，一步一步分析，逐步得出正确答案。需指出，针对复杂问题，LLM直接给出错误答案的概率比较高。思维链可以看成是一种指令微调。
