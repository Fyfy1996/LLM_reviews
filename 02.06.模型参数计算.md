https://zhuanlan.zhihu.com/p/643829565

采用decoder-only框架transformer模型的模型参数量、计算量、中间激活值、KV cache。

为了方便分析，先定义好一些数学符号。记transformer模型的层数为 $l$ ，隐藏层维度为 $h$ ，注意力头数为 $a$ 。词表大小为 $V$ ，训练数据的批次大小为 $b$ ，序列长度为 $s$ 。

transformer模型由 $l$ 个相同的层组成，每个层分为两部分：self-attention块和MLP块。

# 参数量

### self-attention块的参数量

包含$W_Q$,$W_K$,$W_V$三个权重矩阵和偏置，每个权重矩阵形状是$[h,h]$,偏置形状是$[h,1]$

**Self-attention参数量$4h^2+4h**

### MLP块

包含两个线性层。

一般地，

第一个线性层是先将维度从 $h$ 映射到 $4h$ ，第一个线性层的权重矩阵 $W1$ 的形状为 $[h,4h]$ ，偏置的形状为 $[4h]$

第二个线性层再将维度从$4h$映射到$h$。 权重矩阵 $W2$ 的形状为 $[4h,h]$ ，偏置形状为 $[h] $。

**MLP块的参数量为 $8h^2+5h $**

### layer normalization

self-attention块和MLP块各包含了2个可训练模型参数：缩放参数 γ 和平移参数 β ，形状都是 $[h]$ 。2个layer normalization的参数量为 $4h$ 。

总的，**每个transformer层的参数量**为 $12h^2+13h$

### 词嵌入

词向量通常等于隐藏维度$h$,词嵌入矩阵的参数量为$Vh$

### 总结

**l层transformer的可训练模型参数量为$l(12h^2+13h)+Vh$**

*当h较大时，近似$12lh^2$**

### 不同版本LLaMA模型


| 实际参数量 | 隐藏维度h | 层数l | 近似值 12lh^2 |
| ------------ | ----------- | ------- | ---------------- |
| 6.7B       | 4096      | 32    | 6,442,450,944  |
| 13.0B      | 5120      | 40    | 12,582,912,000 |
| 32.5B      | 6656      | 60    | 31,897,681,920 |
| 65.2B      | 8192      | 80    | 64,424,509,440 |

# 显存占用分析

## 训练显存占用
